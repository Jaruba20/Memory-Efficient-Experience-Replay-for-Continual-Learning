{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with memory efficient experience replay for Streaming Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from continualai.colab.scripts import mnist\n",
    "mnist.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, t_train, x_test, t_test = mnist.load()\n",
    "\n",
    "print(\"x_train dim and type: \", x_train.shape, x_train.dtype)\n",
    "print(\"t_train dim and type: \", t_train.shape, t_train.dtype)\n",
    "print(\"x_test dim and type: \", x_test.shape, x_test.dtype)\n",
    "print(\"t_test dim and type: \", t_test.shape, t_test.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(2,2)\n",
    "axarr[0,0].imshow(x_train[1, 0], cmap=\"gray\")\n",
    "axarr[0,1].imshow(x_train[2, 0], cmap=\"gray\")\n",
    "axarr[1,0].imshow(x_train[3, 0], cmap=\"gray\")\n",
    "axarr[1,1].imshow(x_train[4, 0], cmap=\"gray\")\n",
    "np.vectorize(lambda ax:ax.axis('off'))(axarr);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch to False to use CPU\n",
    "use_cuda = True\n",
    "\n",
    "use_cuda = use_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\");\n",
    "torch.manual_seed(1); # Sets the seed for generating random numbers on all devices. Returns a torch.Generator object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5) # 1 input channel. Outputs 10 channels of 24x24 images (28 (pixels) - 5(kernel) + 1 (stride)) = 24\n",
    "                                                     # Max pooling of 2x2 reduces the size to 12x12\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5) # 10 input channels. Outputs 20 channels of 8x8 images (12 - 5 + 1) = 8\n",
    "                                                        # Max pooling of 2x2 reduces the size to 4x4    \n",
    "        self.conv2_drop = nn.Dropout2d() # A 2D dropout layer which zeros out entire channels of an input feature map.\n",
    "        self.fc1 = nn.Linear(320, 50) # 320 = 20 * 4 * 4\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, x_train, t_train, optimizer, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    for start in range(0, len(t_train)-1, 256):\n",
    "      end = start + 256\n",
    "      x, y = torch.from_numpy(x_train[start:end]), torch.from_numpy(t_train[start:end]).long()\n",
    "      x, y = x.to(device), y.to(device)\n",
    "      \n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      output = model(x)\n",
    "      loss = F.cross_entropy(output, y)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      #print(loss.item())\n",
    "    print('Train Epoch: {} \\tLoss: {:.6f}'.format(epoch, loss.item()))\n",
    "\n",
    "def test(model, device, x_test, t_test):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for start in range(0, len(t_test)-1, 256):\n",
    "      end = start + 256\n",
    "      with torch.no_grad():\n",
    "        x, y = torch.from_numpy(x_test[start:end]), torch.from_numpy(t_test[start:end]).long()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        output = model(x)\n",
    "        test_loss += F.cross_entropy(output, y).item() # sum up batch loss\n",
    "        pred = output.max(1, keepdim=True)[1] # get the index of the max logit\n",
    "        correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(t_test)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(t_test),\n",
    "        100. * correct / len(t_test)))\n",
    "    return 100. * correct / len(t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_mnist(mnist, seed):\n",
    "    \"\"\" Given the training set, permute pixels of each img the same way. \"\"\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    print(\"starting permutation...\")\n",
    "    h = w = 28\n",
    "    perm_inds = list(range(h*w))\n",
    "    np.random.shuffle(perm_inds)\n",
    "    # print(perm_inds)\n",
    "    perm_mnist = []\n",
    "    for set in mnist:\n",
    "        num_img = set.shape[0]\n",
    "        flat_set = set.reshape(num_img, w * h)\n",
    "        perm_mnist.append(flat_set[:, perm_inds].reshape(num_img, 1, w, h))\n",
    "    print(\"done.\")\n",
    "    return perm_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 1\n",
    "task_1 = [(x_train, t_train), (x_test, t_test)]\n",
    "\n",
    "# task 2\n",
    "x_train2, x_test2 = permute_mnist([x_train, x_test], 1)\n",
    "task_2 = [(x_train2, t_train), (x_test2, t_test)]\n",
    "\n",
    "# task 3\n",
    "x_train3, x_test3 = permute_mnist([x_train, x_test], 2)\n",
    "task_3 = [(x_train3, t_train), (x_test3, t_test)]\n",
    "\n",
    "# task list\n",
    "tasks = [task_1, task_2, task_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_in_unison(dataset, seed, in_place=False):\n",
    "    \"\"\" Shuffle two (or more) list in unison. \"\"\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    rng_state = np.random.get_state()\n",
    "    new_dataset = []\n",
    "    for x in dataset:\n",
    "        if in_place:\n",
    "            np.random.shuffle(x)\n",
    "        else:\n",
    "            new_dataset.append(np.random.permutation(x))\n",
    "        np.random.set_state(rng_state)\n",
    "\n",
    "    if not in_place:\n",
    "        return new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plan: Use a buffer to store a small amount of data from the previous task.\n",
    "#       This buffer will be used to train the model on the current task.\n",
    "#       The buffer will be updated with new data from the current task.\n",
    "#       The model will be trained on the current task and the buffer.\n",
    "#       This process will be repeated for all tasks.\n",
    "#       We will have k class-specific buffers, one for each class.\n",
    "#       Each buffer will store at most c samples.\n",
    "#       When a buffer is full, the oldest sample will be removed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
